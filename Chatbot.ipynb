{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggFp0DD-o98y",
        "outputId": "20b734d4-81d3-49b6-d850-64e4c8fb7432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SJSVQWz55cj"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfyLa4Vb52kt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from googlesearch import search\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Take input a disease and return the content of wikipedia's infobox for that specific disease\n",
        "\n",
        "def diseaseDetail(term):\n",
        "    diseases=[term]\n",
        "    ret=term+\"\\n\"\n",
        "    for dis in diseases:\n",
        "        # search \"disease wikipedia\" on google \n",
        "        query = dis+' wikipedia'\n",
        "        for sr in search(query,tld=\"co.in\",stop=10,pause=0.5): \n",
        "            # open wikipedia link\n",
        "            match=re.search(r'wikipedia',sr)\n",
        "            filled = 0\n",
        "            if match:\n",
        "                wiki = requests.get(sr,verify=False)\n",
        "                soup = BeautifulSoup(wiki.content, 'html5lib')\n",
        "                # Fetch HTML code for 'infobox'\n",
        "                info_table = soup.find(\"table\", {\"class\":\"infobox\"})\n",
        "                if info_table is not None:\n",
        "                    # Preprocess contents of infobox\n",
        "                    for row in info_table.find_all(\"tr\"):\n",
        "                        data=row.find(\"th\",{\"scope\":\"row\"})\n",
        "                        if data is not None:\n",
        "                            symptom=str(row.find(\"td\"))\n",
        "                            symptom = symptom.replace('.','')\n",
        "                            symptom = symptom.replace(';',',')\n",
        "                            symptom = symptom.replace('<b>','<b> \\n')\n",
        "                            symptom=re.sub(r'<a.*?>','',symptom) # Remove hyperlink\n",
        "                            symptom=re.sub(r'</a>','',symptom) # Remove hyperlink\n",
        "                            symptom=re.sub(r'<[^<]+?>',' ',symptom) # All the tags\n",
        "                            symptom=re.sub(r'\\[.*\\]','',symptom) # Remove citation text                     \n",
        "                            symptom=symptom.replace(\"&gt\",\">\")\n",
        "                            ret+=data.get_text()+\" - \"+symptom+\"\\n\"\n",
        "#                            print(data.get_text(),\"-\",symptom)\n",
        "                            filled = 1\n",
        "                if filled:\n",
        "                    break\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyuIsC526G8P",
        "outputId": "583d36df-ce56-415b-b080-ee19ce1ce5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from statistics import mean\n",
        "from nltk.corpus import wordnet \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from itertools import combinations\n",
        "from time import time\n",
        "from collections import Counter\n",
        "import operator\n",
        "from xgboost import XGBClassifier\n",
        "import math\n",
        "#from Treatment import diseaseDetail\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4PlXkQkGFp4d"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "splitter = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImaviQKM6s2z"
      },
      "outputs": [],
      "source": [
        "def synonyms(term):\n",
        "    synonyms = []\n",
        "    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
        "    soup = BeautifulSoup(response.content,  \"html.parser\")\n",
        "    try:\n",
        "        container=soup.find('section', {'class': 'MainContentContainer'}) \n",
        "        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})\n",
        "        row = row.find_all('li')\n",
        "        for x in row:\n",
        "            synonyms.append(x.get_text())\n",
        "    except:\n",
        "        None\n",
        "    for syn in wordnet.synsets(term):\n",
        "        synonyms+=syn.lemma_names()\n",
        "    return set(synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVPxtSvO65Eu"
      },
      "outputs": [],
      "source": [
        "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
        "# Scrapping and creation of dataset csv is done in a separate program\n",
        "df_comb = pd.read_csv(\"/content/gdrive/MyDrive/dis_sym_dataset_comb.csv\") # Disease combination\n",
        "df_norm = pd.read_csv(\"/content/gdrive/MyDrive/dis_sym_dataset_norm.csv\") # Individual Disease\n",
        "\n",
        "X = df_comb.iloc[:, 1:]\n",
        "Y = df_comb.iloc[:, 0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70fB5Ehh8Dbn"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "lr = lr.fit(X, Y)\n",
        "scores = cross_val_score(lr, X, Y, cv=5)\n",
        "# MLP Classifier\n",
        "#from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='tanh', solver='adam', max_iter=100)\n",
        "mlp = mlp.fit(X, Y)\n",
        "scores_mlp = cross_val_score(mlp, X, Y, cv=5)\n",
        "# RF Classifier\n",
        "rf = RandomForestClassifier(n_estimators=10, criterion='entropy')\n",
        "rf = rf.fit(X, Y)\n",
        "scores_rf = cross_val_score(rf, X, Y, cv=5)\n",
        "# KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=31, weights='distance', n_jobs=4)\n",
        "knn = knn.fit(X, Y)\n",
        "scores_knn = cross_val_score(knn, X, Y, cv=5)\n",
        "# DT Classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt = dt.fit(X, Y)\n",
        "scores_dt = cross_val_score(dt, X, Y, cv=5)\n",
        "# SVM Classifier\n",
        "svm = SVC()\n",
        "svm = svm.fit(X, Y)\n",
        "scores_svm = cross_val_score(svm, X, Y, cv=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThCYmjwn9T42"
      },
      "outputs": [],
      "source": [
        "X = df_norm.iloc[:, 1:]\n",
        "Y = df_norm.iloc[:, 0:1]\n",
        "# List of symptoms\n",
        "dataset_symptoms = list(X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGeQWztC9bEE",
        "outputId": "9e138663-0811-4670-d021-eb6878f7264a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter symptoms separated by comma(,):\n",
            "Increased thirst, Frequent urination,Extreme hunger, Unexplained weight loss\n"
          ]
        }
      ],
      "source": [
        "# Taking symptoms from user as input \n",
        "# coughing,pyrexia,tire,loss of smell\n",
        "user_symptoms = str(input(\"Please enter symptoms separated by comma(,):\\n\")).lower().split(',')\n",
        "# Preprocessing the input symptoms\n",
        "processed_user_symptoms=[]\n",
        "for sym in user_symptoms:\n",
        "    sym=sym.strip()\n",
        "    sym=sym.replace('-',' ')\n",
        "    sym=sym.replace(\"'\",'')\n",
        "    sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])\n",
        "    processed_user_symptoms.append(sym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UljHoyrqEBgq"
      },
      "outputs": [],
      "source": [
        "# Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string\n",
        "user_symptoms = []\n",
        "for user_sym in processed_user_symptoms:\n",
        "    user_sym = user_sym.split()\n",
        "    str_sym = set()\n",
        "    for comb in range(1, len(user_sym)+1):\n",
        "        for subset in combinations(user_sym, comb):\n",
        "            subset=' '.join(subset)\n",
        "            subset = synonyms(subset) \n",
        "            str_sym.update(subset)\n",
        "    str_sym.add(' '.join(user_sym))\n",
        "    user_symptoms.append(' '.join(str_sym).replace('_',' '))\n",
        "# query expansion performed by joining synonyms found for each symptoms initially entered\n",
        "##print(\"After query expansion done by using the symptoms entered\")\n",
        "#print(user_symptoms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaTeThWtGHa5"
      },
      "outputs": [],
      "source": [
        "# Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input \n",
        "# symptoms. If similarity>0.5, add the symptom to the final list\n",
        "found_symptoms = set()\n",
        "for idx, data_sym in enumerate(dataset_symptoms):\n",
        "    data_sym_split=data_sym.split()\n",
        "    for user_sym in user_symptoms:\n",
        "        count=0\n",
        "        for symp in data_sym_split:\n",
        "            if symp in user_sym.split():\n",
        "                count+=1\n",
        "        if count/len(data_sym_split)>0.5:\n",
        "            found_symptoms.add(data_sym)\n",
        "found_symptoms = list(found_symptoms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZu7SEg_GRo1",
        "outputId": "e4093a42-020e-4848-9c08-8cd0fb4b84ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symptoms that best match your search!\n",
            "0 : increased hunger\n",
            "1 : unexplained weight loss\n",
            "2 : unintended weight loss\n",
            "3 : frequent urination\n",
            "4 : increased thirst\n",
            "5 : red\n",
            "\n",
            "Please select all the relevant symptoms. Enter indices (separated-space):\n",
            "0 1 2 3 4\n"
          ]
        }
      ],
      "source": [
        "# Print all found symptoms\n",
        "print(\"Symptoms that best match your search!\")\n",
        "for idx, symp in enumerate(found_symptoms):\n",
        "    print(idx,\":\",symp)\n",
        "    \n",
        "# Show the related symptoms found in the dataset and ask user to select among them\n",
        "select_list = input(\"\\nPlease select all the relevant symptoms. Enter indices (separated-space):\\n\").split()\n",
        "\n",
        "# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the\n",
        "# ones that is input by the user\n",
        "dis_list = set()\n",
        "final_symp = [] \n",
        "counter_list = []\n",
        "for idx in select_list:\n",
        "    symp=found_symptoms[int(idx)]\n",
        "    final_symp.append(symp)\n",
        "    dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))\n",
        "   \n",
        "for dis in dis_list:\n",
        "    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n",
        "    row[0].pop(0)\n",
        "    for idx,val in enumerate(row[0]):\n",
        "        if val!=0 and dataset_symptoms[idx] not in final_symp:\n",
        "            counter_list.append(dataset_symptoms[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmZfkm6tGz-f"
      },
      "outputs": [],
      "source": [
        "# Symptoms that co-occur with the ones selected by user              \n",
        "dict_symp = dict(Counter(counter_list))\n",
        "dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)   \n",
        "#print(dict_symp_tup) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwk4sPtlG8KI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e91cc85-247b-4cc8-cffc-aff2073028cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : fever\n",
            "1 : testicular pain\n",
            "2 : fast heartbeat\n",
            "3 : irritability\n",
            "4 : muscle weakness\n",
            "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "1\n",
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : sleeping problem\n",
            "1 : change bowel movement\n",
            "2 : missed period\n",
            "3 : nausea vomiting\n",
            "4 : tender breast\n",
            "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "no\n"
          ]
        }
      ],
      "source": [
        "# Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable \n",
        "found_symptoms=[]\n",
        "count=0\n",
        "for tup in dict_symp_tup:\n",
        "    count+=1\n",
        "    found_symptoms.append(tup[0])\n",
        "    if count%5==0 or count==len(dict_symp_tup):\n",
        "        print(\"\\nCommon co-occuring symptoms:\")\n",
        "        for idx,ele in enumerate(found_symptoms):\n",
        "            print(idx,\":\",ele)\n",
        "        select_list = input(\"Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\\n\").lower().split();\n",
        "        if select_list[0]=='no':\n",
        "            break\n",
        "        if select_list[0]=='-1':\n",
        "            found_symptoms = [] \n",
        "            continue\n",
        "        for idx in select_list:\n",
        "            final_symp.append(found_symptoms[int(idx)])\n",
        "        found_symptoms = [] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPh1vFgaHXZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b375345b-01ab-4d4c-8277-55fc342ce507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final list of Symptoms that will be used for prediction:\n",
            "increased hunger\n",
            "unexplained weight loss\n",
            "unintended weight loss\n",
            "frequent urination\n",
            "increased thirst\n",
            "testicular pain\n"
          ]
        }
      ],
      "source": [
        "# Create query vector based on symptoms selected by the user\n",
        "print(\"\\nFinal list of Symptoms that will be used for prediction:\")\n",
        "sample_x = [0 for x in range(0,len(dataset_symptoms))]\n",
        "for val in final_symp:\n",
        "    print(val)\n",
        "    sample_x[dataset_symptoms.index(val)]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWujBwghHdlu"
      },
      "outputs": [],
      "source": [
        "# Predict disease\n",
        "#lr\n",
        "lr = LogisticRegression()\n",
        "lr = lr.fit(X, Y)\n",
        "prediction_lr = lr.predict_proba([sample_x])\n",
        "#mlp\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='tanh', solver='adam', max_iter=100)\n",
        "mlp = mlp.fit(X, Y)\n",
        "prediction_mlp = mlp.predict_proba([sample_x])\n",
        "#rf\n",
        "rf = RandomForestClassifier(n_estimators=10, criterion='entropy')\n",
        "rf = rf.fit(X, Y)\n",
        "prediction_rf = rf.predict_proba([sample_x])\n",
        "# KNN \n",
        "knn = KNeighborsClassifier(n_neighbors=31, weights='distance', n_jobs=4)\n",
        "knn = knn.fit(X, Y)\n",
        "prediction_knn = knn.predict_proba([sample_x])\n",
        "# DT Classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt = dt.fit(X, Y)\n",
        "prediction_dt = dt.predict_proba([sample_x])\n",
        "# SVM Classifier\n",
        "svm = SVC()\n",
        "svm = svm.fit(X, Y)\n",
        "prediction_svm = svm.predict([sample_x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-POD1Xg7eKq"
      },
      "outputs": [],
      "source": [
        "# k = 4\n",
        "# print(\"prediction_lr[0]:\",prediction_lr[0].argsort()[-k:][::-1])\n",
        "# print(\"prediction_mlp[0]:\",prediction_mlp[0].argsort()[-k:][::-1])\n",
        "# print(\"prediction_knn[0]:\",prediction_knn[0].argsort()[-k:][::-1])\n",
        "# print(\"prediction_dt[0]:\",prediction_dt[0].argsort()[-k:][::-1])\n",
        "# print(\"prediction_rf[0]:\",prediction_rf[0].argsort()[-k:][::-1])\n",
        "# print(\"prediction_svm:\",prediction_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekqKGnhnpK9F"
      },
      "outputs": [],
      "source": [
        "#mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='tanh', solver='adam', max_iter=100)\n",
        "#mlp = mlp.fit(X, Y)\n",
        "#prediction = mlp.predict_proba([sample_x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZQdf9QFHnox"
      },
      "outputs": [],
      "source": [
        "k = 1\n",
        "diseases = list(set(Y['label_dis']))\n",
        "diseases.sort()\n",
        "topk_lr = prediction_lr[0].argsort()[-k:][::-1]\n",
        "topk_mlp = prediction_mlp[0].argsort()[-k:][::-1]\n",
        "topk_knn = prediction_knn[0].argsort()[-k:][::-1]\n",
        "topk_dt = prediction_dt[0].argsort()[-k:][::-1]\n",
        "topk_rf = prediction_rf[0].argsort()[-k:][::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ENeaO_RAAEz"
      },
      "outputs": [],
      "source": [
        "def diseasemap(topk):\n",
        "  topk_dict = {}\n",
        "  for idx,t in  enumerate(topk):\n",
        "    match_sym=set()\n",
        "    row = df_norm.loc[df_norm['label_dis'] == diseases[t]].values.tolist()\n",
        "    row[0].pop(0)\n",
        "\n",
        "    for idx,val in enumerate(row[0]):\n",
        "        if val!=0:\n",
        "            match_sym.add(dataset_symptoms[idx])\n",
        "    prob = (len(match_sym.intersection(set(final_symp)))+1)/(len(set(final_symp))+1)\n",
        "    prob *= mean(scores_mlp)\n",
        "    topk_dict[t] = prob\n",
        "\n",
        "  j = 0\n",
        "  topk_sorted = dict(sorted(topk_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "  topk_index_mapping = []\n",
        "  for key in topk_sorted:\n",
        "    prob = topk_sorted[key]*100\n",
        "    # print(str(j) + \" Disease name:\",diseases[key], \"\\tProbability:\",str(round(prob, 2))+\"%\")\n",
        "    topk_index_mapping.append(diseases[key])\n",
        "    j += 1\n",
        "\n",
        "  return topk_index_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs6BA1FYD41s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f386564-62e9-4d58-f0c4-6b9a4309adbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top disease predicted based on symptoms by 6 Models\n",
            "Diabetes Mellitus\n"
          ]
        }
      ],
      "source": [
        "pred = diseasemap(topk_knn)+diseasemap(topk_lr)+diseasemap(topk_rf)+diseasemap(topk_dt)+diseasemap(topk_mlp)+prediction_svm.tolist()\n",
        "pred = Counter(pred)\n",
        "predicted_disease = sorted(pred.items(), key=lambda kv:(kv[1], kv[0]),reverse = True)[:1][0][0]\n",
        "print(f\"\\nTop disease predicted based on symptoms by 6 Models\")\n",
        "print(predicted_disease)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNEy7LGoHw7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba83646-edb8-4e76-8679-23addf34bac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "More details about the disease? Enter '1' for continue or '-1' to discontinue and close the system:\n",
            "1\n",
            "\n",
            "Diabetes Mellitus\n",
            "Pronunciation -       /  ˌ  d  aɪ       ə  ˈ  b  i       t  i  z  ,   -  t  ɪ  s  /     /  ˈ  m  ɛ  l       ɪ       t  ə  s  ,     m  ə  ˈ  l  aɪ  -/       \n",
            "Specialty -  Endocrinology \n",
            "Symptoms -   \n",
            "  Frequent urination \n",
            " increased thirst \n",
            " increased hunger    \n",
            "  \n",
            "Complications -  Diabetic ketoacidosis, hyperosmolar hyperglycemic state, heart disease, stroke, pain/pins and needles in hands and/or feet, chronic kidney failure, foot ulcers, cognitive impairment, gastroparesis   \n",
            "Risk factors -    \n",
            "Type 1 : Family history     \n",
            "Type 2 : Obesity, lack of exercise, genetics,   \n",
            "Diagnostic method -  High blood sugar   \n",
            "Treatment -   \n",
            "  Healthy diet \n",
            " physical exercise    \n",
            "  \n",
            "Medication -  Insulin, anti-diabetic medication like metformin   \n",
            "Frequency -  463 million (88%)   \n",
            "Deaths -  42 million (2019)   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#print(f\"\\nTop disease predicted based on symptoms by 6 Models\")\n",
        "select = input(\"\\nMore details about the disease? Enter '1' for continue or '-1' to discontinue and close the system:\\n\")\n",
        "if select!='-1':\n",
        "    print()\n",
        "    print(diseaseDetail(predicted_disease))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-ahvIfASF-D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}